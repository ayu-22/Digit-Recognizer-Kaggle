{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "digit_new.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQWpslLQaovf",
        "colab_type": "code",
        "outputId": "15e57110-62bb-405e-a1b7-c0e9f97622bb",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 93
        }
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7d1624ca-6818-434b-b06b-362780c5ecfc\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-7d1624ca-6818-434b-b06b-362780c5ecfc\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"ayusingh22\",\"key\":\"9100f8d48652377c0b5e5b8763432a27\"}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YfKu_U-Pc0y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kC3BeD4XPiQo",
        "colab_type": "code",
        "outputId": "3b54e919-e092-4266-fb68-dccdcd42e529",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        }
      },
      "source": [
        "!kaggle competitions download -c digit-recognizer\n",
        "!unzip test.zip\n",
        "!unzip train.zip\n",
        "!unzip sample_submission.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Downloading train.csv to /content\n",
            " 78% 57.0M/73.2M [00:00<00:00, 134MB/s]\n",
            "100% 73.2M/73.2M [00:00<00:00, 136MB/s]\n",
            "Downloading test.csv to /content\n",
            " 88% 43.0M/48.8M [00:00<00:00, 187MB/s]\n",
            "100% 48.8M/48.8M [00:00<00:00, 193MB/s]\n",
            "Downloading sample_submission.csv to /content\n",
            "  0% 0.00/235k [00:00<?, ?B/s]\n",
            "100% 235k/235k [00:00<00:00, 211MB/s]\n",
            "unzip:  cannot find or open test.zip, test.zip.zip or test.zip.ZIP.\n",
            "unzip:  cannot find or open train.zip, train.zip.zip or train.zip.ZIP.\n",
            "unzip:  cannot find or open sample_submission.zip, sample_submission.zip.zip or sample_submission.zip.ZIP.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fa1RdsrPj37",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "import shutil\n",
        "import keras\n",
        "from keras.models import Model,Sequential\n",
        "from keras.layers import *\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.datasets import cifar10\n",
        "from keras import regularizers\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "import numpy as np\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPooling2D\n",
        "from keras.layers import Dropout, Flatten, GlobalAveragePooling2D\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import ndimage\n",
        "from keras.callbacks import Callback,ModelCheckpoint\n",
        "from keras.models import Sequential,load_model\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "import keras.backend as K\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import scipy.ndimage"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zG3V3C_cP_P-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train=pd.read_csv(\"train.csv\")\n",
        "test=pd.read_csv(\"test.csv\")\n",
        "sample=pd.read_csv(\"sample_submission.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEyrmbT4QDgd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels=train[\"label\"]\n",
        "train.drop(columns=[\"label\"],inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnVQPO63QHAX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train=np.empty((42000,28,28,1),np.uint8)\n",
        "for i in range(42000):\n",
        "  x_train[i]=np.array(train.iloc[[i]]).reshape(28,28,1)\n",
        "  \n",
        "x_test=np.empty((28000,28,28,1),np.uint8)\n",
        "for i in range(28000):\n",
        "  x_test[i]=np.array(test.iloc[[i]]).reshape(28,28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnHL2jyiXODH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y=pd.get_dummies(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OlP-MsVQX9h",
        "colab_type": "code",
        "outputId": "bab99e30-38e1-4d87-aca7-89510e0ae795",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "plt.imshow(x_train[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f5202b94f98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4_HlxyAV0K4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_tr, x_te, y_tr, y_te = train_test_split(x_train, y, test_size=0.1, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UC1t3mugQ8Xq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import Sequence\n",
        "import math\n",
        "class datagen(Sequence):\n",
        "  def __init__(self,labels,batch_size,image_size):\n",
        "    self.target=labels\n",
        "    self.batch_length=batch_size\n",
        "    self.image_length=image_size\n",
        "    self.size=len(labels)\n",
        "  def __getitem__(self,i):\n",
        "    start=i*self.batch_length\n",
        "    end=min((i+1)*self.batch_length,self.size)\n",
        "    train_img=np.empty(((end-start)*5,self.image_length,self.image_length,1),'uint8')\n",
        "    a=0\n",
        "    for x in range(start,end):\n",
        "      img = x_tr[x]\n",
        "      num_x = np.random.randint(0,3)\n",
        "      num_y = np.random.randint(0,3)\n",
        "      img_ran = img[num_x:num_x+24, num_y:num_y+24]\n",
        "      rot_ran = np.random.randint(1,10)\n",
        "      rot_ran_neg = np.random.randint(1,10)\n",
        "      rot_img_pos = ndimage.rotate(img_ran, rot_ran)\n",
        "      rot_img_neg = ndimage.rotate(img_ran, -rot_ran_neg)\n",
        "      rot_pos = cv2.resize(rot_img_pos,(24,24)).reshape((24,24,1))\n",
        "      rot_neg = cv2.resize(rot_img_neg,(24,24)).reshape((24,24,1))\n",
        "      rows,cols,d = img_ran.shape\n",
        "      ran_shift_x = np.random.randint(1,3)\n",
        "      ran_shift_y = np.random.randint(1,3)\n",
        "      M = np.float32([[1,0,ran_shift_x],[0,1,ran_shift_y]])\n",
        "      img_shift = cv2.warpAffine(img,M,(cols,rows)).reshape((24,24,1))\n",
        "      ran_shift_x_new = np.random.randint(1,3)\n",
        "      ran_shift_y_new = np.random.randint(1,3)\n",
        "      img_sh_base = cv2.resize(img,(24,24))\n",
        "      M = np.float32([[1,0,ran_shift_x],[0,1,ran_shift_y]])\n",
        "      img_shift_new = cv2.warpAffine(img_sh_base,M,(cols,rows)).reshape((24,24,1))\n",
        "      \n",
        "        \n",
        "      \n",
        "      train_img[a] = img_ran\n",
        "      train_img[a + end-start] = rot_pos\n",
        "      train_img[a + 2*(end-start)] = rot_neg\n",
        "      train_img[a + 3*(end-start)] = img_shift\n",
        "      train_img[a + 4*(end-start)] = img_shift_new\n",
        "      \n",
        "      a=a+1\n",
        "      \n",
        "    Y_train=self.target[start:end]\n",
        "    y_train=np.concatenate((Y_train,Y_train,Y_train,Y_train,Y_train),axis=0)\n",
        "    return (train_img,y_train)\n",
        "  def __len__(self):\n",
        "    return math.ceil(self.size/self.batch_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NkJ6zvKVGOw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import Sequence\n",
        "import math\n",
        "class datagen1(Sequence):\n",
        "    \n",
        "  def __init__(self,labels,batch_size,image_size):\n",
        "    self.target=labels\n",
        "    self.batch_length=batch_size\n",
        "    self.image_length=image_size\n",
        "    self.size=len(labels)\n",
        "  def __getitem__(self,i):\n",
        "    start=i*self.batch_length\n",
        "    end=min((i+1)*self.batch_length,self.size)\n",
        "    train_img=np.empty((end-start,self.image_length,self.image_length,1),'uint8')\n",
        "    a=0\n",
        "    for x in range(start,end):\n",
        "      img = x_te[x]\n",
        "      img = cv2.resize(img, (24,24)).reshape((24,24,1))\n",
        "      train_img[a]=img\n",
        "      a=a+1\n",
        "    y_train=self.target[start:end]\n",
        "    \n",
        "    return (train_img,y_train)\n",
        "  def __len__(self):\n",
        "    return math.ceil(self.size/self.batch_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-a0RYXzXVdn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "traingen=datagen(y_tr,64,24)\n",
        "valgen=datagen1(y_te,64,24)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLX9gRi0XsQF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
        "                                            patience=2, \n",
        "                                            verbose=1, \n",
        "                                            factor=0.1, \n",
        "                                            min_lr=0.000000001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnJqkuhznSGP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.utils import class_weight\n",
        "class_weights = class_weight.compute_class_weight('balanced',\n",
        "                                                 np.unique(labels),\n",
        "                                                 labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAzVCFr2Xvyb",
        "colab_type": "code",
        "outputId": "962dec2a-a90b-4fd7-8947-673b987c68c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1407
        }
      },
      "source": [
        "weight_decay = 1e-4\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=(24,24,1)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        " \n",
        "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        " \n",
        "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        " \n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        " \n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_23 (Conv2D)           (None, 24, 24, 32)        320       \n",
            "_________________________________________________________________\n",
            "activation_19 (Activation)   (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_22 (Batc (None, 24, 24, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_24 (Conv2D)           (None, 24, 24, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_20 (Activation)   (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_23 (Batc (None, 24, 24, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_25 (Conv2D)           (None, 24, 24, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_21 (Activation)   (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_24 (Batc (None, 24, 24, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 12, 12, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_26 (Conv2D)           (None, 12, 12, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_22 (Activation)   (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_25 (Batc (None, 12, 12, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_27 (Conv2D)           (None, 12, 12, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_23 (Activation)   (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_26 (Batc (None, 12, 12, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_28 (Conv2D)           (None, 12, 12, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_24 (Activation)   (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_27 (Batc (None, 12, 12, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_29 (Conv2D)           (None, 6, 6, 128)         73856     \n",
            "_________________________________________________________________\n",
            "activation_25 (Activation)   (None, 6, 6, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_28 (Batc (None, 6, 6, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_30 (Conv2D)           (None, 6, 6, 128)         147584    \n",
            "_________________________________________________________________\n",
            "activation_26 (Activation)   (None, 6, 6, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_29 (Batc (None, 6, 6, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_31 (Conv2D)           (None, 6, 6, 64)          73792     \n",
            "_________________________________________________________________\n",
            "activation_27 (Activation)   (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_30 (Batc (None, 6, 6, 64)          256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 3, 3, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 576)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 256)               147712    \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 64)                16448     \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 573,642\n",
            "Trainable params: 572,426\n",
            "Non-trainable params: 1,216\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkKM9WX8X-UR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "model.compile(loss='categorical_crossentropy',optimizer=adam ,metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InEKutZfYIiT",
        "colab_type": "code",
        "outputId": "6f1f9b4c-c3eb-453f-bb7c-440c28725681",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1689
        }
      },
      "source": [
        "model.fit_generator(traingen,\n",
        "                   validation_data=valgen,\n",
        "                   epochs=40,\n",
        "                   shuffle=True,\n",
        "                   verbose=1,\n",
        "                   callbacks=[learning_rate_reduction],\n",
        "                   class_weight = class_weights\n",
        "                   )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "591/591 [==============================] - 35s 60ms/step - loss: 0.4224 - acc: 0.8918 - val_loss: 0.1174 - val_acc: 0.9836\n",
            "Epoch 2/40\n",
            "591/591 [==============================] - 30s 51ms/step - loss: 0.1551 - acc: 0.9790 - val_loss: 0.1356 - val_acc: 0.9821\n",
            "Epoch 3/40\n",
            "591/591 [==============================] - 30s 51ms/step - loss: 0.1322 - acc: 0.9837 - val_loss: 0.1311 - val_acc: 0.9814\n",
            "\n",
            "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Epoch 4/40\n",
            "591/591 [==============================] - 30s 50ms/step - loss: 0.0939 - acc: 0.9924 - val_loss: 0.0823 - val_acc: 0.9938\n",
            "Epoch 5/40\n",
            "591/591 [==============================] - 30s 51ms/step - loss: 0.0858 - acc: 0.9941 - val_loss: 0.0821 - val_acc: 0.9929\n",
            "Epoch 6/40\n",
            "591/591 [==============================] - 30s 51ms/step - loss: 0.0822 - acc: 0.9947 - val_loss: 0.0816 - val_acc: 0.9933\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "Epoch 7/40\n",
            "591/591 [==============================] - 30s 51ms/step - loss: 0.0768 - acc: 0.9957 - val_loss: 0.0801 - val_acc: 0.9945\n",
            "Epoch 8/40\n",
            "591/591 [==============================] - 30s 51ms/step - loss: 0.0747 - acc: 0.9962 - val_loss: 0.0799 - val_acc: 0.9943\n",
            "Epoch 9/40\n",
            "591/591 [==============================] - 30s 50ms/step - loss: 0.0748 - acc: 0.9960 - val_loss: 0.0787 - val_acc: 0.9943\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "Epoch 10/40\n",
            "591/591 [==============================] - 30s 51ms/step - loss: 0.0742 - acc: 0.9961 - val_loss: 0.0789 - val_acc: 0.9943\n",
            "Epoch 11/40\n",
            "591/591 [==============================] - 30s 50ms/step - loss: 0.0736 - acc: 0.9963 - val_loss: 0.0788 - val_acc: 0.9943\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
            "Epoch 12/40\n",
            "591/591 [==============================] - 30s 50ms/step - loss: 0.0737 - acc: 0.9964 - val_loss: 0.0789 - val_acc: 0.9943\n",
            "Epoch 13/40\n",
            "591/591 [==============================] - 30s 51ms/step - loss: 0.0739 - acc: 0.9962 - val_loss: 0.0790 - val_acc: 0.9943\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
            "Epoch 14/40\n",
            "591/591 [==============================] - 29s 50ms/step - loss: 0.0732 - acc: 0.9963 - val_loss: 0.0790 - val_acc: 0.9943\n",
            "Epoch 15/40\n",
            "591/591 [==============================] - 30s 50ms/step - loss: 0.0734 - acc: 0.9963 - val_loss: 0.0790 - val_acc: 0.9943\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
            "Epoch 16/40\n",
            "591/591 [==============================] - 29s 50ms/step - loss: 0.0737 - acc: 0.9964 - val_loss: 0.0788 - val_acc: 0.9943\n",
            "Epoch 17/40\n",
            "591/591 [==============================] - 29s 50ms/step - loss: 0.0733 - acc: 0.9963 - val_loss: 0.0790 - val_acc: 0.9943\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1e-09.\n",
            "Epoch 18/40\n",
            "591/591 [==============================] - 30s 50ms/step - loss: 0.0740 - acc: 0.9961 - val_loss: 0.0788 - val_acc: 0.9943\n",
            "Epoch 19/40\n",
            "591/591 [==============================] - 29s 50ms/step - loss: 0.0735 - acc: 0.9963 - val_loss: 0.0790 - val_acc: 0.9943\n",
            "Epoch 20/40\n",
            "591/591 [==============================] - 30s 50ms/step - loss: 0.0737 - acc: 0.9962 - val_loss: 0.0789 - val_acc: 0.9943\n",
            "Epoch 21/40\n",
            "591/591 [==============================] - 29s 50ms/step - loss: 0.0737 - acc: 0.9963 - val_loss: 0.0789 - val_acc: 0.9943\n",
            "Epoch 22/40\n",
            "591/591 [==============================] - 30s 50ms/step - loss: 0.0729 - acc: 0.9964 - val_loss: 0.0790 - val_acc: 0.9943\n",
            "Epoch 23/40\n",
            "591/591 [==============================] - 29s 50ms/step - loss: 0.0734 - acc: 0.9963 - val_loss: 0.0790 - val_acc: 0.9943\n",
            "Epoch 24/40\n",
            "591/591 [==============================] - 29s 50ms/step - loss: 0.0741 - acc: 0.9963 - val_loss: 0.0790 - val_acc: 0.9943\n",
            "Epoch 25/40\n",
            "591/591 [==============================] - 29s 50ms/step - loss: 0.0736 - acc: 0.9963 - val_loss: 0.0789 - val_acc: 0.9943\n",
            "Epoch 26/40\n",
            "591/591 [==============================] - 31s 52ms/step - loss: 0.0736 - acc: 0.9962 - val_loss: 0.0789 - val_acc: 0.9943\n",
            "Epoch 27/40\n",
            "591/591 [==============================] - 30s 51ms/step - loss: 0.0740 - acc: 0.9962 - val_loss: 0.0790 - val_acc: 0.9943\n",
            "Epoch 28/40\n",
            "591/591 [==============================] - 30s 50ms/step - loss: 0.0737 - acc: 0.9963 - val_loss: 0.0788 - val_acc: 0.9943\n",
            "Epoch 29/40\n",
            "591/591 [==============================] - 30s 51ms/step - loss: 0.0734 - acc: 0.9962 - val_loss: 0.0789 - val_acc: 0.9943\n",
            "Epoch 30/40\n",
            "591/591 [==============================] - 30s 50ms/step - loss: 0.0738 - acc: 0.9962 - val_loss: 0.0790 - val_acc: 0.9943\n",
            "Epoch 31/40\n",
            "591/591 [==============================] - 32s 54ms/step - loss: 0.0737 - acc: 0.9963 - val_loss: 0.0791 - val_acc: 0.9943\n",
            "Epoch 32/40\n",
            "591/591 [==============================] - 30s 50ms/step - loss: 0.0734 - acc: 0.9962 - val_loss: 0.0790 - val_acc: 0.9943\n",
            "Epoch 33/40\n",
            "591/591 [==============================] - 29s 49ms/step - loss: 0.0736 - acc: 0.9963 - val_loss: 0.0789 - val_acc: 0.9943\n",
            "Epoch 34/40\n",
            "591/591 [==============================] - 30s 50ms/step - loss: 0.0737 - acc: 0.9961 - val_loss: 0.0789 - val_acc: 0.9943\n",
            "Epoch 35/40\n",
            "591/591 [==============================] - 30s 50ms/step - loss: 0.0733 - acc: 0.9964 - val_loss: 0.0790 - val_acc: 0.9943\n",
            "Epoch 36/40\n",
            "591/591 [==============================] - 29s 50ms/step - loss: 0.0738 - acc: 0.9964 - val_loss: 0.0789 - val_acc: 0.9943\n",
            "Epoch 37/40\n",
            "591/591 [==============================] - 30s 50ms/step - loss: 0.0738 - acc: 0.9962 - val_loss: 0.0790 - val_acc: 0.9943\n",
            "Epoch 38/40\n",
            "591/591 [==============================] - 30s 50ms/step - loss: 0.0740 - acc: 0.9962 - val_loss: 0.0791 - val_acc: 0.9943\n",
            "Epoch 39/40\n",
            "591/591 [==============================] - 31s 52ms/step - loss: 0.0735 - acc: 0.9964 - val_loss: 0.0789 - val_acc: 0.9943\n",
            "Epoch 40/40\n",
            "591/591 [==============================] - 31s 52ms/step - loss: 0.0737 - acc: 0.9964 - val_loss: 0.0790 - val_acc: 0.9943\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f517c238da0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQOJxGjrDyXm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_weights('digit.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ncQpCRGYNw_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test1 = np.empty((28000,24,24,1),np.uint8)\n",
        "x_test2 = np.empty((28000,24,24,1),np.uint8)\n",
        "x_test3 = np.empty((28000,24,24,1),np.uint8)\n",
        "x_test4 = np.empty((28000,24,24,1),np.uint8)\n",
        "x_test5 = np.empty((28000,24,24,1),np.uint8)\n",
        "x_test6 = np.empty((28000,24,24,1),np.uint8)\n",
        "\n",
        "for i in range(28000):\n",
        "  img = x_test[i]\n",
        "  num_x = np.random.randint(0,3)\n",
        "  num_y = np.random.randint(0,3)\n",
        "  im1 = cv2.resize(img,(24,24)).reshape((24,24,1))\n",
        "  img_crop1 = img[num_x:num_x+24, num_y:num_y+24]\n",
        "  num_x2 = np.random.randint(0,3)\n",
        "  num_y2 = np.random.randint(0,3)\n",
        "  img_crop2 = img[num_x2:num_x2+24, num_y2:num_y2+24]\n",
        "  rot_ran_neg = np.random.randint(1,10)\n",
        "  rot_img_neg = ndimage.rotate(img, -rot_ran_neg)\n",
        "  rot_img_neg = cv2.resize(rot_img_neg,(24,24)).reshape((24,24,1))\n",
        "  \n",
        "  rot_ran = np.random.randint(1,10)\n",
        "  rot_img = ndimage.rotate(img, rot_ran)\n",
        "  rot_img = cv2.resize(rot_img,(24,24)).reshape((24,24,1))\n",
        "\n",
        "  \n",
        "  ran_shift_x = np.random.randint(1,10)\n",
        "  ran_shift_y = np.random.randint(1,10)                   \n",
        "  M = np.float32([[1,0,ran_shift_x],[0,1,ran_shift_y]])\n",
        "  img_shift = cv2.warpAffine(img,M,(24,24)).reshape((24,24,1))\n",
        "                     \n",
        "                     \n",
        "  x_test1[i] = im1\n",
        "  x_test2[i] = img_crop1\n",
        "  x_test3[i] = img_crop2\n",
        "  x_test4[i] = rot_img_neg\n",
        "  x_test5[i] = rot_img\n",
        "  x_test6[i] = img_shift\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXBDsheXwt_1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pre1 = model.predict(x_test1)\n",
        "y_pre2 = model.predict(x_test2)\n",
        "y_pre3 = model.predict(x_test3)\n",
        "y_pre4 = model.predict(x_test4)\n",
        "y_pre5 = model.predict(x_test5)\n",
        "y_pre6 = model.predict(x_test6)\n",
        "\n",
        "y_pre = np.mean((\n",
        "    y_pre1,y_pre2,y_pre3,y_pre4,y_pre5,y_pre6 \n",
        "                ), axis = 0)\n",
        "y_pre=np.argmax(y_pre,axis=1)\n",
        "y_pre = y_pre\n",
        "sample['Label'] = y_pre\n",
        "sample.to_csv('submission_digit.csv', index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgrlGco8y9k6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}